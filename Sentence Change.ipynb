{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from InferSent.models import InferSent\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from random import choice\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.spatial import distance\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `InferSent` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'InferSent/encoder/infersent2.pkl'\n",
    "params = {'bsize': 256, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "            'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "model = InferSent(params).cuda()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = 'InferSent/dataset/fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/mnt/bigfiles/dl/datasets/Gutenberg/'\n",
    "books = os.listdir(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(author):\n",
    "    corpus = ''\n",
    "    \n",
    "    for book in books:\n",
    "        if f'{author}__' in book:\n",
    "            corpus += open(prefix + book).read() + '\\n\\n'\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE = '<NEWLINE>'\n",
    "def tokenize_sentences(text):\n",
    "    open('/tmp/in.txt', 'w').write(text.replace('\\n\\n', NEWLINE))\n",
    "    os.system('/mnt/bigfiles/dl/datasets/stanford-parser-full-2017-06-09/tokenize_sent.sh')\n",
    "    tokens = open('/tmp/out.txt').read().split('\\n')\n",
    "    print('Total tokens in dataset', len(tokens))\n",
    "\n",
    "    return [token for token in tokens if len(token) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_sentences(sentences):\n",
    "    open('/tmp/in.txt', 'w').write(' '.join(sentences).replace(NEWLINE, '\\n\\n'))\n",
    "    os.system('/mnt/bigfiles/dl/datasets/stanford-parser-full-2017-06-09/detokenize.sh')\n",
    "    \n",
    "    return open('/tmp/out.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_book(toChange, source, useAnnoy = False, maxChars = 1000000):\n",
    "    toChangeSent = tokenize_sentences(toChange)\n",
    "    sourceSent = tokenize_sentences(source[:maxChars])\n",
    "    \n",
    "    model.build_vocab(toChangeSent + sourceSent, tokenize=True)\n",
    "    \n",
    "    toChangeVec = model.encode(toChangeSent, tokenize=True)\n",
    "    sourceVec = model.encode(sourceSent, tokenize=True)\n",
    "    \n",
    "    changed = []\n",
    "    \n",
    "    if useAnnoy:\n",
    "        print('Building index...')\n",
    "        index = AnnoyIndex(len(sourceVec[0]), metric='dot')\n",
    "        for (i, vec) in enumerate(sourceVec):\n",
    "            index.add_item(i, vec)\n",
    "        index.build(25)\n",
    "\n",
    "        for lineVec in tqdm(toChangeVec):\n",
    "            closestIdx = index.get_nns_by_vector(lineVec, 1)[0]\n",
    "            changed.append(sourceSent[closestIdx])\n",
    "    else:\n",
    "        for lineVec in tqdm(toChangeVec):\n",
    "            distances = [distance.cosine(lineVec, possibleVec) for possibleVec in sourceVec]\n",
    "            closestIdx = np.argmin(distances)\n",
    "            changed.append(sourceSent[closestIdx])\n",
    "            \n",
    "    \n",
    "    return detokenize_sentences(changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in dataset 3615\n",
      "Total tokens in dataset 10678\n"
     ]
    }
   ],
   "source": [
    "changed = change_book(open(prefix + 'Jane Austen___Northanger Abbey.txt').read(), get_corpus('Sir Arthur Conan Doyle'))\n",
    "# change_book('I like to do things. Things are fun.', 'Things are fun. I like to do stuff.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
